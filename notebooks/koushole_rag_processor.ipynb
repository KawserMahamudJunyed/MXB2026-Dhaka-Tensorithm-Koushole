{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ Koushole RAG Processor (EasyOCR Edition)\n",
                "\n",
                "**Process NCTB textbooks for RAG using FREE cloud GPU**\n",
                "\n",
                "**Version:** Stable (EasyOCR) + Voyage AI (Optimized Mode) + **ToC Support**\n",
                "\n",
                "---\n",
                "\n",
                "**SETUP:**\n",
                "1. Go to Runtime ‚Üí Change runtime type ‚Üí Select **T4 GPU**\n",
                "2. Click **Runtime ‚Üí Restart session** (CRITICAL)\n",
                "3. Run all cells in order\n",
                "4. Enter API keys when prompted"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 1Ô∏è‚É£ Install Dependencies\n",
                "!pip install -q easyocr pdf2image Pillow supabase voyageai\n",
                "!apt-get install -q poppler-utils\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 2Ô∏è‚É£ Enter Credentials\n",
                "from getpass import getpass\n",
                "\n",
                "print(\"üîë Enter your credentials (hidden):\")\n",
                "SUPABASE_URL = input(\"Supabase URL: \")\n",
                "SUPABASE_KEY = getpass(\"Supabase Service Key: \")\n",
                "VOYAGE_API_KEY = getpass(\"Voyage AI API Key: \")\n",
                "\n",
                "if not SUPABASE_URL or not SUPABASE_KEY or not VOYAGE_API_KEY:\n",
                "    raise ValueError(\"‚ùå All credentials are required!\")\n",
                "print(\"‚úÖ Credentials saved.\")"
            ],
            "metadata": {
                "id": "keys"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 3Ô∏è‚É£ Initialize Clients & Models\n",
                "from supabase import create_client\n",
                "import voyageai\n",
                "import sys\n",
                "import easyocr\n",
                "\n",
                "# 1. Setup Clients\n",
                "try:\n",
                "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
                "    voyage = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
                "    print(\"‚úÖ Clients initialized\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Client error: {e}\")\n",
                "    sys.exit(1)\n",
                "\n",
                "# 2. Setup OCR\n",
                "print(\"\\nüîÆ detailed setup for EasyOCR (GPU)...\")\n",
                "try:\n",
                "    # Initialize EasyOCR Reader for Bangla (bn) and English (en)\n",
                "    reader = easyocr.Reader(['bn', 'en'], gpu=True)\n",
                "    print(\"‚úÖ EasyOCR Model Loaded Successfully\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Model Loading Error: {e}\")\n",
                "    sys.exit(1)"
            ],
            "metadata": {
                "id": "setup_models"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 4Ô∏è‚É£ Helper Functions (With ToC Logic)\n",
                "from pdf2image import convert_from_bytes\n",
                "import requests\n",
                "from tqdm import tqdm\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import time\n",
                "import re\n",
                "\n",
                "def extract_toc_easyocr(pdf_bytes, max_pages_to_scan=15):\n",
                "    \"\"\"Scan first few pages for Table of Contents\"\"\"\n",
                "    print(\"\\n  üïµÔ∏è Scanning for Table of Contents...\")\n",
                "    try:\n",
                "        images = convert_from_bytes(pdf_bytes, first_page=1, last_page=max_pages_to_scan, dpi=150)\n",
                "        chapters = []\n",
                "        \n",
                "        # Regex: matches 'Title ..... 123' or 'Title 123' (English or Bangla digits)\n",
                "        # Basic pattern: Ends with digits. \n",
                "        # Note: Bangla digits range from \\u09E6 to \\u09EF\n",
                "        toc_pattern = re.compile(r\"^(.*?)[\\.\\s\\‚Ä¶]+([0-9]+|[‡ß¶-‡ßØ]+)$\")\n",
                "        \n",
                "        # Simple cleaner for Bangla digits -> English int\n",
                "        bn_map = str.maketrans('‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ', '0123456789')\n",
                "        \n",
                "        for page_idx, img in enumerate(images):\n",
                "            img_np = np.array(img)\n",
                "            result = reader.readtext(img_np, detail=0)\n",
                "            \n",
                "            for line in result:\n",
                "                match = toc_pattern.match(line.strip())\n",
                "                if match:\n",
                "                    title = match.group(1).strip()\n",
                "                    page_str = match.group(2).strip().translate(bn_map)\n",
                "                    \n",
                "                    # Basic validation: Title length > 2, Page is a number\n",
                "                    if len(title) < 3 or not page_str.isdigit(): \n",
                "                        continue\n",
                "                    \n",
                "                    page_num = int(page_str)\n",
                "                    \n",
                "                    # Monotonicity Check: If this page < last chapter page, it might be noise OR sub-chapter\n",
                "                    # For simplicity, we accept it if it's reasonable.\n",
                "                    chapters.append({\n",
                "                        'chapter_number': len(chapters) + 1,\n",
                "                        'title': title,\n",
                "                        'start_page': page_num\n",
                "                    })\n",
                "                        \n",
                "        print(f\"  ‚úÖ Found {len(chapters)} chapters\")\n",
                "        # Sort chapters by page number just in case\n",
                "        chapters.sort(key=lambda x: x['start_page'])\n",
                "        return chapters\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è ToC scan failed (skipping): {e}\")\n",
                "        return []\n",
                "\n",
                "def save_chapters_to_db(book_id, chapters, source_type):\n",
                "    \"\"\"Save chapters and return a lookup map: {start_page: chapter_id}\"\"\"\n",
                "    if not chapters:\n",
                "        return {}\n",
                "    \n",
                "    id_col = 'library_book_id' if source_type == 'library' else 'resource_id'\n",
                "    db_rows = []\n",
                "    for ch in chapters:\n",
                "        db_rows.append({\n",
                "            id_col: book_id,\n",
                "            'chapter_number': ch['chapter_number'],\n",
                "            'title': ch['title'],\n",
                "            'start_page': ch['start_page']\n",
                "        })\n",
                "    \n",
                "    try:\n",
                "        # Insert and return inserted rows (we need their UUIDs)\n",
                "        res = supabase.table('book_chapters').insert(db_rows).execute()\n",
                "        inserted = res.data or []\n",
                "        \n",
                "        # Build a map for easy lookup: page -> chapter_id\n",
                "        page_map = {}\n",
                "        for row in inserted:\n",
                "            page_map[row['start_page']] = row['id']\n",
                "            \n",
                "        return page_map\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è Failed to save chapters: {e}\")\n",
                "        return {}\n",
                "\n",
                "def extract_text_and_chunk_with_chapters(pdf_bytes, chapter_map):\n",
                "    \"\"\"Processing pipeline: OCR -> Assign Chapter -> Chunk\"\"\"\n",
                "    try:\n",
                "        images = convert_from_bytes(pdf_bytes, dpi=200)\n",
                "        chunks = []\n",
                "        \n",
                "        # Convert map keys to sorted list for range checking\n",
                "        sorted_start_pages = sorted(chapter_map.keys())\n",
                "        current_chapter_id = None\n",
                "        \n",
                "        pbar = tqdm(total=len(images), desc=\"OCR & Chunking\")\n",
                "        \n",
                "        for page_num, img in enumerate(images, start=1):\n",
                "            # 1. Determine Chapter\n",
                "            # If current page matches a chapter start, switch to it\n",
                "            if page_num in chapter_map:\n",
                "                current_chapter_id = chapter_map[page_num]\n",
                "            # Else, find the latest chapter start <= page_num\n",
                "            elif not current_chapter_id and sorted_start_pages:\n",
                "                 # Handle case where book starts before chapter 1 (Intro)\n",
                "                 # or we missed the exact start page. Find closest previous.\n",
                "                 valid_starts = [p for p in sorted_start_pages if p <= page_num]\n",
                "                 if valid_starts:\n",
                "                     current_chapter_id = chapter_map[valid_starts[-1]]\n",
                "\n",
                "            # 2. OCR\n",
                "            img_np = np.array(img)\n",
                "            result = reader.readtext(img_np, detail=0, paragraph=True)\n",
                "            page_text = \"\\n\".join(result)\n",
                "            \n",
                "            if not page_text.strip(): \n",
                "                pbar.update(1)\n",
                "                continue\n",
                "\n",
                "            # 3. Chunk this page immediately\n",
                "            # We treat page boundaries as natural chunk breaks to keep chapters clean\n",
                "            page_chunks = split_into_chunks(page_text, 1000, 200)\n",
                "            \n",
                "            for txt in page_chunks:\n",
                "                chunks.append({\n",
                "                    'text': txt,\n",
                "                    'chapter_id': current_chapter_id\n",
                "                })\n",
                "                \n",
                "            pbar.update(1)\n",
                "            \n",
                "        pbar.close()\n",
                "        return chunks\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Processing Failed: {e}\")\n",
                "        return []\n",
                "\n",
                "def split_into_chunks(text, chunk_size, overlap):\n",
                "    out = []\n",
                "    start = 0\n",
                "    while start < len(text):\n",
                "        end = start + chunk_size\n",
                "        chunk = text[start:end]\n",
                "        if chunk.strip():\n",
                "            out.append(chunk.strip())\n",
                "        start = end - overlap\n",
                "        if start >= len(text) - overlap:\n",
                "            break\n",
                "    return [c for c in out if len(c) > 50]\n",
                "\n",
                "def generate_embeddings(chunks_with_metadata):\n",
                "    \"\"\"Embed chunks (accepts dicts, extracts text)\"\"\"\n",
                "    if not chunks_with_metadata: return []\n",
                "    embeddings = []\n",
                "    \n",
                "    batch_size = 4\n",
                "    print(f\"  üß† Generating embeddings for {len(chunks_with_metadata)} chunks...\")\n",
                "    \n",
                "    for i in tqdm(range(0, len(chunks_with_metadata), batch_size), desc=\"Embedding\"):\n",
                "        # Extract just the text for embedding\n",
                "        batch_dicts = chunks_with_metadata[i:i+batch_size]\n",
                "        batch_texts = [c['text'][:8000] for c in batch_dicts]\n",
                "        \n",
                "        if i > 0: time.sleep(25) \n",
                "        \n",
                "        retries = 3\n",
                "        for attempt in range(retries):\n",
                "            try:\n",
                "                res = voyage.embed(batch_texts, model=\"voyage-multilingual-2\", input_type=\"document\")\n",
                "                embeddings.extend(res.embeddings)\n",
                "                break \n",
                "            except Exception as e:\n",
                "                err_msg = str(e).lower()\n",
                "                if \"rate limit\" in err_msg or \"429\" in err_msg:\n",
                "                    time.sleep(75 + (attempt * 15))\n",
                "                else:\n",
                "                    raise e\n",
                "        else:\n",
                "            raise Exception(\"‚ùå Max retries exceeded.\")\n",
                "            \n",
                "    return embeddings\n",
                "\n",
                "print(\"‚úÖ Helper functions ready!\")"
            ],
            "metadata": {
                "id": "helpers"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 5Ô∏è‚É£ Fetch Pending Books\n",
                "print(\"üì° Fetching pending books from Supabase...\")\n",
                "\n",
                "# Get library books\n",
                "lib_res = supabase.table('library_books').select('id, title, file_url').or_('chunks_generated.is.null,chunks_generated.eq.false').execute()\n",
                "lib_books = lib_res.data or []\n",
                "\n",
                "# Get official books\n",
                "off_res = supabase.table('official_resources').select('id, title, file_url').or_('chunks_generated.is.null,chunks_generated.eq.false').execute()\n",
                "off_books = off_res.data or []\n",
                "\n",
                "all_books = [(b, 'library') for b in lib_books] + [(b, 'official') for b in off_books]\n",
                "\n",
                "print(f\"üìö Library books: {len(lib_books)}\")\n",
                "print(f\"üìñ Official books: {len(off_books)}\")\n",
                "print(f\"üìä Total to process: {len(all_books)}\")"
            ],
            "metadata": {
                "id": "fetch"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 6Ô∏è‚É£ Process Loop üöÄ (Integrated)\n",
                "import time\n",
                "\n",
                "def process_book(book, source_type):\n",
                "    title = book['title']\n",
                "    id_col = 'library_book_id' if source_type == 'library' else 'resource_id'\n",
                "    print(f\"\\nüìò Processing: {title}\")\n",
                "    \n",
                "    # 0. CLEANUP \n",
                "    print(\"  üßπ Cleaning old data...\")\n",
                "    try:\n",
                "        supabase.table('book_chunks').delete().eq(id_col, book['id']).execute()\n",
                "        supabase.table('book_chapters').delete().eq(id_col, book['id']).execute()\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ö†Ô∏è Cleanup warning: {e}\")\n",
                "\n",
                "    # 1. Download\n",
                "    try:\n",
                "        print(\"  üì• Downloading...\")\n",
                "        r = requests.get(book['file_url'])\n",
                "        if r.status_code != 200:\n",
                "            print(\"  ‚ùå Download failed\")\n",
                "            return False\n",
                "        pdf_bytes = r.content\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Download error: {e}\")\n",
                "        return False\n",
                "\n",
                "    # 2. ToC Extraction\n",
                "    print(\"  üîé Extracting Chapters...\")\n",
                "    chapters = extract_toc_easyocr(pdf_bytes)\n",
                "    \n",
                "    # 3. Save Chapters & Get ID Map\n",
                "    chapter_map = save_chapters_to_db(book['id'], chapters, source_type)\n",
                "    if chapter_map:\n",
                "        print(f\"  ‚úÖ Saved {len(chapter_map)} chapters to DB\")\n",
                "    else:\n",
                "        print(\"  ‚ÑπÔ∏è No chapters found, proceeding with flat structure.\")\n",
                "\n",
                "    # 4. OCR & Chunking (Page-Aware)\n",
                "    start_time = time.time()\n",
                "    # Returns list of dicts: {'text': '...', 'chapter_id': '...'}\n",
                "    chunks_data = extract_text_and_chunk_with_chapters(pdf_bytes, chapter_map)\n",
                "    ocr_time = time.time() - start_time\n",
                "    \n",
                "    if not chunks_data:\n",
                "        print(\"  ‚ùå No text extracted\")\n",
                "        return False\n",
                "    print(f\"  ‚úÖ Generated {len(chunks_data)} chunks in {ocr_time:.1f}s\")\n",
                "\n",
                "    # 5. Embed\n",
                "    try:\n",
                "        embeddings = generate_embeddings(chunks_data)\n",
                "        if len(embeddings) != len(chunks_data):\n",
                "            print(\"  ‚ùå Embeddings mismatch\")\n",
                "            return False\n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Embedding error: {e}\")\n",
                "        return False\n",
                "\n",
                "    # 6. Store\n",
                "    print(\"  üíæ saving chunks to databse...\")\n",
                "    try:\n",
                "        batch = []\n",
                "        for idx, (c_data, emb) in enumerate(zip(chunks_data, embeddings)):\n",
                "            batch.append({\n",
                "                id_col: book['id'],\n",
                "                'chunk_index': idx,\n",
                "                'chunk_text': c_data['text'],\n",
                "                'chapter_id': c_data['chapter_id'], # CRITICAL LINK\n",
                "                'embedding': emb\n",
                "            })\n",
                "            if len(batch) >= 50:\n",
                "                supabase.table('book_chunks').insert(batch).execute()\n",
                "                batch = []\n",
                "        if batch:\n",
                "            supabase.table('book_chunks').insert(batch).execute()\n",
                "            \n",
                "        # Update Status\n",
                "        table = 'library_books' if source_type == 'library' else 'official_resources'\n",
                "        supabase.table(table).update({\n",
                "            'chunks_generated': True,\n",
                "            'total_chunks': len(chunks_data),\n",
                "            'is_processed': True\n",
                "        }).eq('id', book['id']).execute()\n",
                "        \n",
                "        print(\"  ‚úÖ SUCCESS!\")\n",
                "        return True\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"  ‚ùå Database error: {e}\")\n",
                "        return False\n",
                "\n",
                "# --- MAIN LOOP ---\n",
                "success_count = 0\n",
                "fail_count = 0\n",
                "\n",
                "for book, source in all_books:\n",
                "    if process_book(book, source):\n",
                "        success_count += 1\n",
                "    else:\n",
                "        fail_count += 1\n",
                "    time.sleep(1)\n",
                "\n",
                "print(f\"\\n{'='*40}\")\n",
                "print(f\"üèÅ DONE! Success: {success_count}, Failed: {fail_count}\")"
            ],
            "metadata": {
                "id": "runner"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üéâ All Done!\n",
                "\n",
                "Check your website chat - the new books should now be searchable!"
            ],
            "metadata": {
                "id": "done"
            }
        }
    ]
}