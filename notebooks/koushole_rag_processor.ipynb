{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ Koushole RAG Processor\n",
                "\n",
                "**Process NCTB textbooks for RAG using FREE cloud GPU**\n",
                "\n",
                "This notebook is designed to be **robust** across Surya OCR versions.\n",
                "\n",
                "---\n",
                "\n",
                "**SETUP:**\n",
                "1. Go to Runtime ‚Üí Change runtime type ‚Üí Select **T4 GPU**\n",
                "2. Run all cells in order\n",
                "3. If any error occurs, check the debug output"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 1Ô∏è‚É£ Install Dependencies\n",
                "# We install the latest versions to ensure compatibility\n",
                "!pip install -q --upgrade surya-ocr pdf2image Pillow supabase voyageai transformers torch\n",
                "!apt-get install -q poppler-utils\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 2Ô∏è‚É£ Check Environment & Versions\n",
                "import importlib\n",
                "import pkg_resources\n",
                "\n",
                "packages = ['surya', 'transformers', 'torch', 'voyageai', 'supabase']\n",
                "print(\"üîç Checking versions...\")\n",
                "for package in packages:\n",
                "    try:\n",
                "        ver = pkg_resources.get_distribution(package if package != 'surya' else 'surya-ocr').version\n",
                "        print(f\"  - {package}: {ver}\")\n",
                "    except Exception as e:\n",
                "        print(f\"  - {package}: Not found or error ({e})\")\n"
            ],
            "metadata": {
                "id": "check_versions"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 3Ô∏è‚É£ Enter Credentials\n",
                "from getpass import getpass\n",
                "\n",
                "print(\"üîë Enter your credentials (hidden):\")\n",
                "SUPABASE_URL = input(\"Supabase URL: \")\n",
                "SUPABASE_KEY = getpass(\"Supabase Service Key: \")\n",
                "VOYAGE_API_KEY = getpass(\"Voyage AI API Key: \")\n",
                "\n",
                "if not SUPABASE_URL or not SUPABASE_KEY or not VOYAGE_API_KEY:\n",
                "    raise ValueError(\"‚ùå All credentials are required!\")\n",
                "print(\"‚úÖ Credentials saved.\")"
            ],
            "metadata": {
                "id": "creds"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 4Ô∏è‚É£ Initialize Clients & Models (Robust Loader)\n",
                "from supabase import create_client\n",
                "import voyageai\n",
                "import inspect\n",
                "import sys\n",
                "\n",
                "# 1. Setup Clients\n",
                "try:\n",
                "    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
                "    voyage = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
                "    print(\"‚úÖ Clients initialized\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Client error: {e}\")\n",
                "    sys.exit(1)\n",
                "\n",
                "# 2. Setup Surya OCR (Dynamic Import)\n",
                "print(\"\\nüîÆ setting up Surya OCR...\")\n",
                "det_predictor = None\n",
                "rec_predictor = None\n",
                "\n",
                "try:\n",
                "    # Try new API (v0.5+)\n",
                "    from surya.detection import DetectionPredictor\n",
                "    from surya.recognition import RecognitionPredictor\n",
                "    \n",
                "    print(\"  -> Loading DetectionPredictor...\")\n",
                "    det_predictor = DetectionPredictor()\n",
                "    \n",
                "    print(\"  -> Loading RecognitionPredictor...\")\n",
                "    # Check if RecognitionPredictor needs arguments\n",
                "    sig = inspect.signature(RecognitionPredictor.__init__)\n",
                "    params = sig.parameters\n",
                "    \n",
                "    if 'detection_predictor' in params or 'foundation_predictor' in params:\n",
                "        # Some versions require the detector to be passed\n",
                "        print(\"  -> Passing detector to recognition initialization...\")\n",
                "        rec_predictor = RecognitionPredictor(det_predictor)\n",
                "    else:\n",
                "        rec_predictor = RecognitionPredictor()\n",
                "        \n",
                "    print(\"‚úÖ Surya OCR Models Loaded (New API)\")\n",
                "    \n",
                "except ImportError:\n",
                "    # Fallback to Old API\n",
                "    try:\n",
                "        print(\"  ‚ö†Ô∏è New API not found, trying legacy API...\")\n",
                "        from surya.ocr import run_ocr\n",
                "        from surya.model.detection.model import load_model as load_det_model, load_processor as load_det_processor\n",
                "        from surya.model.recognition.model import load_model as load_rec_model, load_processor as load_rec_processor\n",
                "        \n",
                "        det_model = load_det_model()\n",
                "        det_processor = load_det_processor()\n",
                "        rec_model = load_rec_model()\n",
                "        rec_processor = load_rec_processor()\n",
                "        \n",
                "        # Wrap in a compatibility class\n",
                "        class LegacyWrapper:\n",
                "            def run(self, images, langs):\n",
                "                return run_ocr(images, [langs]*len(images), det_model, det_processor, rec_model, rec_processor)\n",
                "        \n",
                "        rec_predictor = LegacyWrapper()\n",
                "        print(\"‚úÖ Surya OCR Models Loaded (Legacy API)\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Failed to load Surya models: {e}\")\n",
                "        print(\"Try running: !pip install transformers==4.36.2 surya-ocr==0.6.0\")\n",
                "        sys.exit(1)\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Model Loading Error: {e}\")\n",
                "    if \"bbox_size\" in str(e):\n",
                "        print(\"\\n‚ö†Ô∏è CRITICAL: Transformers version conflict detected.\")\n",
                "        print(\"Please add a cell above and run: !pip install transformers==4.36.2\")\n",
                "    sys.exit(1)"
            ],
            "metadata": {
                "id": "setup_models"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 5Ô∏è‚É£ Processing Functions\n",
                "from pdf2image import convert_from_bytes\n",
                "import requests\n",
                "from tqdm import tqdm\n",
                "\n",
                "def extract_text_surya(pdf_bytes):\n",
                "    \"\"\"Extract text from PDF using the loaded predictor\"\"\"\n",
                "    try:\n",
                "        images = convert_from_bytes(pdf_bytes, dpi=150)\n",
                "        all_text = []\n",
                "        batch_size = 5\n",
                "        \n",
                "        pbar = tqdm(total=len(images), desc=\"OCR Processing\")\n",
                "        for i in range(0, len(images), batch_size):\n",
                "            batch = images[i:i+batch_size]\n",
                "            \n",
                "            # Dynamic call based on object type\n",
                "            if hasattr(rec_predictor, 'run'):\n",
                "                # Legacy\n",
                "                results = rec_predictor.run(batch, [\"bn\", \"en\"])\n",
                "                for page in results:\n",
                "                    txt = \"\\n\".join([l.text for l in page.text_lines])\n",
                "                    all_text.append(txt)\n",
                "            else:\n",
                "                # New API\n",
                "                # In new API, we might need to separate detection logic if not linked\n",
                "                # But rec_predictor usually handles it if initialized with det_predictor\n",
                "                # Check if it accepts both arguments\n",
                "                \n",
                "                # We'll assume the robust loader set it up correctly to take images\n",
                "                # If it's a RecognitionPredictor, it usually takes images + optional detection results\n",
                "                # If initialized WITH simple detection, we might just pass `image, langs`?\n",
                "                # Let's inspect run method\n",
                "                \n",
                "                # Using the standard flow: Detection -> Recognition\n",
                "                if det_predictor and not hasattr(rec_predictor, 'foundation_predictor'): \n",
                "                    # If they are decoupled, verify signature\n",
                "                    pass\n",
                "                \n",
                "                # Standard execution for v0.6+\n",
                "                # Usually: rec_predictor(images, [langs], detector) if decoupled\n",
                "                # Or if coupled: rec_predictor(images, [langs])?\n",
                "                \n",
                "                # Let's try the safest path: run detection, then recognition\n",
                "                # This works for most recent versions\n",
                "                det_results = det_predictor(batch)  # Run detection\n",
                "                rec_results = rec_predictor(batch, [\"bn\", \"en\"], det_results) # Run recognition with bounds\n",
                "                \n",
                "                for page in rec_results:\n",
                "                    txt = \"\\n\".join([l.text for l in page.text_lines])\n",
                "                    all_text.append(txt)\n",
                "\n",
                "            pbar.update(len(batch))\n",
                "        pbar.close()\n",
                "        return \"\\n\\n\".join(all_text)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"OCR Failed: {e}\")\n",
                "        return \"\"\n",
                "\n",
                "def chunk_text(text, chunk_size=2000, overlap=200):\n",
                "    chunks = []\n",
                "    start = 0\n",
                "    while start < len(text):\n",
                "        end = start + chunk_size\n",
                "        chunk = text[start:end]\n",
                "        if chunk.strip():\n",
                "            chunks.append(chunk.strip())\n",
                "        start = end - overlap\n",
                "        if start >= len(text) - overlap:\n",
                "            break\n",
                "    return [c for c in chunks if len(c) > 50]\n",
                "\n",
                "def generate_embeddings(chunks):\n",
                "    if not chunks: return []\n",
                "    embeddings = []\n",
                "    batch_size = 20\n",
                "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Generating Embeddings\"):\n",
                "        batch = [c[:8000] for c in chunks[i:i+batch_size]]\n",
                "        res = voyage.embed(batch, model=\"voyage-multilingual-2\", input_type=\"document\")\n",
                "        embeddings.extend(res.embeddings)\n",
                "    return embeddings"
            ],
            "metadata": {
                "id": "funcs"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "#@title 6Ô∏è‚É£ Main Loop\n",
                "import time\n",
                "\n",
                "def process_one_book(book, type_tag):\n",
                "    print(f\"\\nüìò [{type_tag}] Processing: {book['title']}\")\n",
                "    \n",
                "    # 1. Download\n",
                "    try:\n",
                "        r = requests.get(book['file_url'])\n",
                "        if r.status_code != 200:\n",
                "            print(\"‚ùå Download failed\")\n",
                "            return False\n",
                "        pdf = r.content\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Download error: {e}\")\n",
                "        return False\n",
                "\n",
                "    # 2. OCR\n",
                "    text = extract_text_surya(pdf)\n",
                "    if not text or len(text) < 100:\n",
                "        print(f\"‚ùå OCR extracting little/no text ({len(text)} chars)\")\n",
                "        return False\n",
                "    print(f\"‚úÖ Extracted {len(text)} characters\")\n",
                "\n",
                "    # 3. Chunk\n",
                "    chunks = chunk_text(text)\n",
                "    print(f\"üì¶ Extracted {len(chunks)} chunks\")\n",
                "\n",
                "    # 4. Embed\n",
                "    try:\n",
                "        embeddings = generate_embeddings(chunks)\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Embedding error: {e}\")\n",
                "        return False\n",
                "\n",
                "    # 5. Store\n",
                "    id_col = 'library_book_id' if type_tag == 'library' else 'resource_id'\n",
                "    \n",
                "    # Cleanup old\n",
                "    supabase.table('book_chunks').delete().eq(id_col, book['id']).execute()\n",
                "    \n",
                "    # Insert new\n",
                "    insert_batch = []\n",
                "    for idx, (txt, emb) in enumerate(zip(chunks, embeddings)):\n",
                "        insert_batch.append({\n",
                "            id_col: book['id'],\n",
                "            'chunk_index': idx,\n",
                "            'chunk_text': txt,\n",
                "            'embedding': emb\n",
                "        })\n",
                "        if len(insert_batch) >= 50:\n",
                "            supabase.table('book_chunks').insert(insert_batch).execute()\n",
                "            insert_batch = []\n",
                "    if insert_batch:\n",
                "        supabase.table('book_chunks').insert(insert_batch).execute()\n",
                "        \n",
                "    # 6. Update Status\n",
                "    table = 'library_books' if type_tag == 'library' else 'official_resources'\n",
                "    supabase.table(table).update({\n",
                "        'chunks_generated': True,\n",
                "        'total_chunks': len(chunks),\n",
                "        'is_processed': True\n",
                "    }).eq('id', book['id']).execute()\n",
                "    \n",
                "    print(\"‚úÖ Saved to DB!\")\n",
                "    return True\n",
                "\n",
                "# --- RUNNER ---\n",
                "print(\"üì° Fetching pending books...\")\n",
                "q1 = supabase.table('library_books').select('id, title, file_url').or_('chunks_generated.is.null,chunks_generated.eq.false').execute()\n",
                "q2 = supabase.table('official_resources').select('id, title, file_url').or_('chunks_generated.is.null,chunks_generated.eq.false').execute()\n",
                "\n",
                "lib_books = q1.data or []\n",
                "off_books = q2.data or []\n",
                "all_books = [(b, 'library') for b in lib_books] + [(b, 'official') for b in off_books]\n",
                "\n",
                "print(f\"üèÅ Found {len(all_books)} books to process.\")\n",
                "\n",
                "for b, tag in all_books:\n",
                "    process_one_book(b, tag)\n",
                "    time.sleep(1)"
            ],
            "metadata": {
                "id": "runner"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}